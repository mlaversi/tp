{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identification\n",
    "\n",
    "### Date: 10/10/2023\n",
    "### Name: LAVERSIN MATHIEU\n",
    "### E-mail: mathieu.laversin@epfedu.fr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: Logistic Regression\n",
    "\n",
    "In this practical exercise, we will work with Logistic Regression.\n",
    "We will use data from Twitter, and try to extract the sentiment of a given tweet (Positive or Negative).\n",
    "\n",
    "This exercise is diveded into the following parts:\n",
    "\n",
    "* Extracting features from text for logistic regression\n",
    "* Implementing logistic regression\n",
    "* Use logistic regression and test it for an NLP example\n",
    "* Analyse the error of the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Needed Downloads\n",
    "\n",
    "We will have to download some datasets for this exercise. Specially, we will be using the [twitter_samples dataset](http://www.nltk.org/howto/twitter.html) from nltk.\n",
    "\n",
    "* You can download this dataset running the following comand in Python:\n",
    "```Python\n",
    "nltk.download('twitter_samples')\n",
    "```\n",
    "\n",
    "We will also use a dateset of stopwords, to help us getting rid of unwanted tokens that would make our model less accurate.\n",
    "\n",
    "* You can download the list of stopwords running the following comand in Python:\n",
    "```python\n",
    "nltk.download('stopwords')\n",
    "```\n",
    "\n",
    "#### You can also use the following functions from the utils.py file:\n",
    "* `process_tweet()`: cleans the text, tokenizes it into separate words, removes stopwords, and converts words to stems.\n",
    "* `build_freqs()`: returns a `freqs` dictionary, which is a python dictionary where each key is a (word,label) tuple. The `value` associated to the key is the count of the occurrences of a `word` (from the key) in the **corpus** (the entire dataset of tweets) that is categorized to the `label` (from the key). The label can either be '1' (positive) or '0' (negative)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell to import nltk\n",
    "import nltk\n",
    "from os import getcwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     /Users/mathieu/nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('twitter_samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/mathieu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import twitter_samples \n",
    "\n",
    "from utils import process_tweet, build_freqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the data\n",
    "* The `twitter_samples` contains subsets of 5,000 positive tweets, 5,000 negative tweets, and the full set of 10,000 tweets.  \n",
    "    * If you used all three datasets, we would introduce duplicates of the positive tweets and negative tweets.  \n",
    "    * You will select just the five thousand positive tweets and five thousand negative tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the set of positive and negative tweets\n",
    "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "all_negative_tweets = twitter_samples.strings('negative_tweets.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Train test split:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into two pieces, one for training and one for testing (validation set) \n",
    "test_pos = all_positive_tweets[4000:]\n",
    "train_pos = all_positive_tweets[:4000]\n",
    "test_neg = all_negative_tweets[4000:]\n",
    "train_neg = all_negative_tweets[:4000]\n",
    "\n",
    "train_x = train_pos + train_neg \n",
    "test_x = test_pos + test_neg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create the numpy array of positive labels and negative labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine positive and negative labels\n",
    "train_y = np.append(np.ones((len(train_pos), 1)), np.zeros((len(train_neg), 1)), axis=0)\n",
    "test_y = np.append(np.ones((len(test_pos), 1)), np.zeros((len(test_neg), 1)), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_y.shape: (8000, 1)\n",
      "test_y.shape: (2000, 1)\n"
     ]
    }
   ],
   "source": [
    "# Print the shape train and test sets\n",
    "print(f'train_y.shape: {str(train_y.shape)}')\n",
    "print(f'test_y.shape: {str(test_y.shape)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the frequency dictionary\n",
    "\n",
    "* You can create the frequency dict using the `build_freqs()` from the provided `utils.py`. Don't forget to check the code and try to understand what it is doing.\n",
    "\n",
    "Loop:\n",
    "\n",
    "```Python\n",
    "    for y,tweet in zip(ys, tweets): # on each tweet\n",
    "        for word in process_tweet(tweet): # on each word of the tweet\n",
    "            pair = (word, y)\n",
    "            if pair in freqs: # if the pair is already a key...\n",
    "                freqs[pair] += 1 # increases the count\n",
    "            else: # if the pair is not yet a key...\n",
    "                freqs[pair] = 1 # adds a new pair to the dict\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(freqs): <class 'dict'>\n",
      "len(freqs): 11340\n"
     ]
    }
   ],
   "source": [
    "# create frequency dictionary\n",
    "freqs = build_freqs(train_x, train_y)\n",
    "\n",
    "# check the output\n",
    "print(f'type(freqs): {str(type(freqs))}')\n",
    "print(f'len(freqs): {str(len(freqs.keys()))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You should get:\n",
    "```\n",
    "type(freqs): <class 'dict'>\n",
    "len(freqs): 11346\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "`process_tweet()` splits the tweet into words, removes stop words and applies stemming. This is also called tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of a positive tweet: #FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n",
      "This is the same tweet after being processed:\n",
      "['followfriday', 'top', 'engag', 'member', 'commun', 'week', ':)']\n"
     ]
    }
   ],
   "source": [
    "# test the function below\n",
    "print(f'Example of a positive tweet: {train_x[0]}')\n",
    "print(f'This is the same tweet after being processed:\\n{process_tweet(train_x[0])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You should get:\n",
    "\n",
    "```\n",
    "This is an example of a positive tweet: \n",
    " #FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n",
    " \n",
    "This is an example of the processes version: \n",
    " ['followfriday', 'top', 'engag', 'member', 'commun', 'week', ':)']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Logistic regression \n",
    "\n",
    "\n",
    "### Part 1.1: Sigmoid\n",
    "Lets use logistic regression to classify sentiments in texts.\n",
    "* Remember the sigmoid function:\n",
    "\n",
    "$$ h(z) = \\frac{1}{1+\\exp^{-z}} \\tag{1}$$\n",
    "\n",
    "The output is a probability value between 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, implement your sigmoid function\n",
    "PS:\n",
    "* This function shall work if it receives `z` as a numpy arrray.\n",
    "* Check: [numpy.exp](https://docs.scipy.org/doc/numpy/reference/generated/numpy.exp.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Graphique of the sigmoid from 0 to 100')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "def sigmoid(z): \n",
    "    #h = []\n",
    "    \n",
    "    '''\n",
    "    Input:\n",
    "        z: iscalar or numpy array\n",
    "    Output:\n",
    "        h: the sigmoid of z\n",
    "    '''\n",
    "\n",
    "    #h.append(1/ 1+ math.exp(-z))\n",
    "\n",
    "    return np.divide(1,1+np.exp(-z))\n",
    "\n",
    "a = sigmoid(np.arange(-5,5,0.1))\n",
    "plt.plot(a)\n",
    "plt.title('Graphique of the sigmoid from 0 to 100')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass.\n",
      "Pass.\n"
     ]
    }
   ],
   "source": [
    "if (sigmoid(0) == 0.5):\n",
    "    print('Pass.')\n",
    "else:\n",
    "    print('Sigmoid Function is badly implemented.')\n",
    "\n",
    "if (sigmoid(4.92) == 0.9927537604041685):\n",
    "    print('Pass.')\n",
    "else:\n",
    "    print('Sigmoid Function is badly implemented.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression: regression and a sigmoid\n",
    "\n",
    "Logistic regression takes a regular linear regression, and applies a sigmoid to the output of the linear regression.\n",
    "\n",
    "Regression:\n",
    "$$z = \\theta_0 x_0 + \\theta_1 x_1 + \\theta_2 x_2 + ... \\theta_N x_N$$\n",
    "Note that the $\\theta$ values are \"weights\". If you took the Deep Learning Specialization, we referred to the weights with the `w` vector.  In this course, we're using a different variable $\\theta$ to refer to the weights.\n",
    "\n",
    "Logistic regression\n",
    "$$ h(z) = \\frac{1}{1+\\exp^{-z}}$$\n",
    "$$z = \\theta_0 x_0 + \\theta_1 x_1 + \\theta_2 x_2 + ... \\theta_N x_N$$\n",
    "We will refer to 'z' as the 'logits'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost function and Gradient\n",
    "\n",
    "The cost function used for logistic regression is the average of the log loss across all training examples:\n",
    "\n",
    "$$J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^m y^{(i)}\\log (h(z(\\theta)^{(i)})) + (1-y^{(i)})\\log (1-h(z(\\theta)^{(i)}))\\tag{5} $$\n",
    "* $m$ is the number of training examples\n",
    "* $y^{(i)}$ is the actual label of the i-th training example.\n",
    "* $h(z(\\theta)^{(i)})$ is the model's prediction for the i-th training example.\n",
    "\n",
    "The loss function for a single training example is\n",
    "$$ Loss = -1 \\times \\left( y^{(i)}\\log (h(z(\\theta)^{(i)})) + (1-y^{(i)})\\log (1-h(z(\\theta)^{(i)})) \\right)$$\n",
    "\n",
    "* All the $h$ values are between 0 and 1, so the logs will be negative. That is the reason for the factor of -1 applied to the sum of the two loss terms.\n",
    "* Note that when the model predicts 1 ($h(z(\\theta)) = 1$) and the label $y$ is also 1, the loss for that training example is 0. \n",
    "* Similarly, when the model predicts 0 ($h(z(\\theta)) = 0$) and the actual label is also 0, the loss for that training example is 0. \n",
    "* However, when the model prediction is close to 1 ($h(z(\\theta)) = 0.9999$) and the label is 0, the second term of the log loss becomes a large negative number, which is then multiplied by the overall factor of -1 to convert it to a positive loss value. $-1 \\times (1 - 0) \\times log(1 - 0.9999) \\approx 9.2$ The closer the model prediction gets to 1, the larger the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.210340371976294"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verify that when the model predicts close to 1, but the actual label is 0, the loss is a large positive value\n",
    "-1 * (1 - 0) * np.log(1 - 0.9999) # loss is about 9.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Likewise, if the model predicts close to 0 ($h(z) = 0.0001$) but the actual label is 1, the first term in the loss function becomes a large number: $-1 \\times log(0.0001) \\approx 9.2$.  The closer the prediction is to zero, the larger the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.210340371976182"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verify that when the model predicts close to 0 but the actual label is 1, the loss is a large positive value\n",
    "-1 * np.log(0.0001) # loss is about 9.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update the weights\n",
    "\n",
    "To update your weight vector $\\theta$, you will apply gradient descent to iteratively improve your model's predictions.  \n",
    "The gradient of the cost function $J$ with respect to one of the weights $\\theta_j$ is:\n",
    "\n",
    "$$\\nabla_{\\theta_j}J(\\theta) = \\frac{1}{m} \\sum_{i=1}^m(h^{(i)}-y^{(i)})x_j \\tag{5}$$\n",
    "* 'i' is the index across all 'm' training examples.\n",
    "* 'j' is the index of the weight $\\theta_j$, so $x_j$ is the feature associated with weight $\\theta_j$\n",
    "\n",
    "* To update the weight $\\theta_j$, we adjust it by subtracting a fraction of the gradient determined by $\\alpha$:\n",
    "$$\\theta_j = \\theta_j - \\alpha \\times \\nabla_{\\theta_j}J(\\theta) $$\n",
    "* The learning rate $\\alpha$ is a value that we choose to control how big a single update will be.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions: Implement gradient descent function\n",
    "* The number of iterations `num_iters` is the number of times that you'll use the entire training set.\n",
    "* For each iteration, you'll calculate the cost function using all training examples (there are `m` training examples), and for all features.\n",
    "* Instead of updating a single weight $\\theta_i$ at a time, we can update all the weights in the column vector:  \n",
    "$$\\mathbf{\\theta} = \\begin{pmatrix}\n",
    "\\theta_0\n",
    "\\\\\n",
    "\\theta_1\n",
    "\\\\ \n",
    "\\theta_2 \n",
    "\\\\ \n",
    "\\vdots\n",
    "\\\\ \n",
    "\\theta_n\n",
    "\\end{pmatrix}$$\n",
    "* $\\mathbf{\\theta}$ has dimensions (n+1, 1), where 'n' is the number of features, and there is one more element for the bias term $\\theta_0$ (note that the corresponding feature value $\\mathbf{x_0}$ is 1).\n",
    "* The 'logits', 'z', are calculated by multiplying the feature matrix 'x' with the weight vector 'theta'.  $z = \\mathbf{x}\\mathbf{\\theta}$\n",
    "    * $\\mathbf{x}$ has dimensions (m, n+1) \n",
    "    * $\\mathbf{\\theta}$: has dimensions (n+1, 1)\n",
    "    * $\\mathbf{z}$: has dimensions (m, 1)\n",
    "* The prediction 'h', is calculated by applying the sigmoid to each element in 'z': $h(z) = sigmoid(z)$, and has dimensions (m,1).\n",
    "* The cost function $J$ is calculated by taking the dot product of the vectors 'y' and 'log(h)'.  Since both 'y' and 'h' are column vectors (m,1), transpose the vector to the left, so that matrix multiplication of a row vector with column vector performs the dot product.\n",
    "$$J = \\frac{-1}{m} \\times \\left(\\mathbf{y}^T \\cdot log(\\mathbf{h}) + \\mathbf{(1-y)}^T \\cdot log(\\mathbf{1-h}) \\right)$$\n",
    "* The update of theta is also vectorized.  Because the dimensions of $\\mathbf{x}$ are (m, n+1), and both $\\mathbf{h}$ and $\\mathbf{y}$ are (m, 1), we need to transpose the $\\mathbf{x}$ and place it on the left in order to perform matrix multiplication, which then yields the (n+1, 1) answer we need:\n",
    "$$\\mathbf{\\theta} = \\mathbf{\\theta} - \\frac{\\alpha}{m} \\times \\left( \\mathbf{x}^T \\cdot \\left( \\mathbf{h-y} \\right) \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, implement you Gradient Descent method:\n",
    "\n",
    "PS:\n",
    "* Use `np.dot` for matrix multiplication.\n",
    "* Make sure that the fraction -1/m is a floating point type. To do so, you can splicitly cast the type, e.g.:`float(1)`, or just `1.0`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost function and Gradient\n",
    "\n",
    "The cost function used for logistic regression is the average of the log loss across all training examples:\n",
    "\n",
    "$$J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^m y^{(i)}\\log (h(z(\\theta)^{(i)})) + (1-y^{(i)})\\log (1-h(z(\\theta)^{(i)}))\\tag{5} $$\n",
    "* $m$ is the number of training examples\n",
    "* $y^{(i)}$ is the actual label of the i-th training example.\n",
    "* $h(z(\\theta)^{(i)})$ is the model's prediction for the i-th training example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescent(x, y, theta, alpha, num_iters):\n",
    "    '''\n",
    "    Input:\n",
    "        x: matrix of features which is (m,n+1)\n",
    "        y: corresponding labels of the input matrix x, dimensions (m,1)\n",
    "        theta: weight vector of dimension (n+1,1)\n",
    "        alpha: learning rate\n",
    "        num_iters: number of iterations for which you want to train your model\n",
    "    Output:\n",
    "        J: the final cost\n",
    "        theta: your final weight vector\n",
    "    Hint: you might want to print the cost to make sure that it is going down.\n",
    "    '''\n",
    "    # get 'm', the number of rows in matrix x\n",
    "    m, _ = x.shape\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "        # get z, the dot product of x and theta\n",
    "        z = np.dot(x, theta)\n",
    "        # get the sigmoid of z\n",
    "        h = sigmoid(z)\n",
    "        # calculate the cost function\n",
    "        J = (-1.0/len(x)) * (np.dot(y.T, np.log(h)) + np.dot((1-y).T, np.log(1-h)))\n",
    "        # update the weights theta\n",
    "        theta -= (alpha/len(x)) *(np.dot(x.T, (h - y)))\n",
    "    J = float(J)\n",
    "\n",
    "    return J, theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after training : 0.67094970.\n",
      "Vector of weights : [4.1e-07, 0.00035658, 7.309e-05]\n"
     ]
    }
   ],
   "source": [
    "# Check the function\n",
    "# Construct a synthetic test case using numpy PRNG functions\n",
    "np.random.seed(1)\n",
    "# X input is 10 x 3 with ones for the bias terms\n",
    "tmp_X = np.append(np.ones((10, 1)), np.random.rand(10, 2) * 2000, axis=1)\n",
    "# Y Labels are 10 x 1\n",
    "tmp_Y = (np.random.rand(10, 1) > 0.35).astype(float)\n",
    "\n",
    "# Apply gradient descent\n",
    "tmp_J, tmp_theta = gradientDescent(tmp_X, tmp_Y, np.zeros((3, 1)), 1e-8, 700)\n",
    "print(f'Cost after training : {tmp_J:.8f}.')\n",
    "print(f'Vector of weights : {[round(t, 8) for t in np.squeeze(tmp_theta)]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You should get:\n",
    "\n",
    "```\n",
    "Cost after training : 0.67094970.\n",
    "Vector of weights : [4.1e-07, 0.00035658, 7.309e-05]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting the features\n",
    "\n",
    "* Given a list of tweets, extract the features and store them in a matrix. You will extract two features.\n",
    "    * The number of positive words in a tweet.\n",
    "    * The number of negative words in a tweet. \n",
    "* Then train your logistic regression classifier on these features.\n",
    "* Test the classifier on a validation set. \n",
    "\n",
    "### Instructions: Implement the extract_features function. \n",
    "* This function takes in a single tweet.\n",
    "* Process the tweet using the imported `process_tweet()` function and save the list of tweet words.\n",
    "* Loop through each word in the list of processed words\n",
    "    * For each word, check the `freqs` dictionary for the count when that word has a positive '1' label. (Check for the key (word, 1.0)\n",
    "    * Do the same for the count for when the word is associated with the negative label '0'. (Check for the key (word, 0.0).)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PS:\n",
    "* You have to handle cases where the (word, label) key is not present in the dictionary. \n",
    "* Hint: check the [.get() method from Python dictionary](https://www.programiz.com/python-programming/methods/dictionary/get)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(tweet, freqs):\n",
    "    '''\n",
    "    Input: \n",
    "        tweet: a list of words for one tweet\n",
    "        freqs: a dictionary corresponding to the frequencies of each tuple (word, label)\n",
    "    Output: \n",
    "        x: a feature vector of dimension (1,3)\n",
    "    '''\n",
    "    # process_tweet tokenizes, stems, and removes stopwords\n",
    "    word_l = process_tweet(tweet)\n",
    "    \n",
    "    # 3 elements in the form of a 1 x 3 vector\n",
    "    x = np.zeros((1, 3)) \n",
    "    \n",
    "    #bias term is set to 1\n",
    "    x[0,0] = 1 \n",
    "        \n",
    "    # loop through each word in the list of words\n",
    "    for word in word_l:\n",
    "        \n",
    "        # increment the word count for the positive label 1\n",
    "        x[0,1] += freqs.get((word,1.0),0)\n",
    "        \n",
    "        # increment the word count for the negative label 0\n",
    "        x[0,2] += freqs.get((word,0.0),0)\n",
    "        \n",
    "    assert(x.shape == (1, 3))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.00e+00 3.02e+03 6.10e+01]]\n"
     ]
    }
   ],
   "source": [
    "# Testing on expected training data\n",
    "tmp1 = extract_features(train_x[0], freqs)\n",
    "print(tmp1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You should get:\n",
    "\n",
    "```\n",
    "[[1.00e+00 3.02e+03 6.10e+01]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Testing for a text with words not present in the dictionary\n",
    "tmp2 = extract_features('bada foo viande moules', freqs)\n",
    "print(tmp2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You should get:\n",
    "\n",
    "```\n",
    "[[1. 0. 0.]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Your Model\n",
    "\n",
    "To train the model:\n",
    "* Put all the features for all training examples into a matrix `X`. \n",
    "* Call the Gradient Descest method `gradientDescent` that you have just programmed.\n",
    "\n",
    "### This section is already programmed. Just go by it and understand what each part of the code is doing.\n",
    "### Explain it in your final report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after training : 0.24216477.\n",
      "Vector of weights : [7e-08, 0.0005239, -0.00055517]\n"
     ]
    }
   ],
   "source": [
    "# collect the features 'x' and stack them into a matrix 'X'\n",
    "X = np.zeros((len(train_x), 3))\n",
    "for i in range(len(train_x)):\n",
    "    X[i, :]= extract_features(train_x[i], freqs)\n",
    "\n",
    "# training labels corresponding to X\n",
    "Y = train_y\n",
    "\n",
    "# Apply gradient descent\n",
    "J, theta = gradientDescent(X, Y, np.zeros((3, 1)), 1e-9, 1500)\n",
    "print(f'Cost after training : {J:.8f}.')\n",
    "print(f'Vector of weights : {[round(t, 8) for t in np.squeeze(theta)]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You should get:\n",
    "\n",
    "```\n",
    "Cost after training : 0.24216529.\n",
    "Vector of weights : [7e-08, 0.0005239, -0.00055517]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test your logistic regression\n",
    "\n",
    "It is time for you to test your logistic regression function on some new input that your model has not seen before. \n",
    "\n",
    "#### Instructions: Write `predict_tweet`\n",
    "Predict whether a tweet is positive or negative.\n",
    "\n",
    "* Given a tweet, process it, then extract the features.\n",
    "* Apply the model's learned weights on the features to get the logits.\n",
    "* Apply the sigmoid to the logits to get the prediction (a value between 0 and 1).\n",
    "\n",
    "$$y_{pred} = sigmoid(\\mathbf{x} \\cdot \\theta)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tweet(tweet, freqs, theta):\n",
    "    '''\n",
    "    Input: \n",
    "        tweet: a string\n",
    "        freqs: a dictionary corresponding to the frequencies of each tuple (word, label)\n",
    "        theta: (3,1) vector of weights\n",
    "    Output: \n",
    "        y_pred: the probability of a tweet being positive or negative\n",
    "    '''\n",
    "    \n",
    "    # extract the features of the tweet and store it into x\n",
    "    x = extract_features(tweet, freqs)\n",
    "    \n",
    "    # make the prediction using x and theta\n",
    "    y_pred = sigmoid(np.dot(x,theta))\n",
    "    \n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am happy -> 0.518580\n",
      "I am bad -> 0.494339\n",
      "this movie should have been great. -> 0.515331\n",
      "great -> 0.515464\n",
      "great great -> 0.530898\n",
      "great great great -> 0.546273\n",
      "great great great great -> 0.561561\n"
     ]
    }
   ],
   "source": [
    "for tweet in ['I am happy', 'I am bad', 'this movie should have been great.', 'great', 'great great', 'great great great', 'great great great great']:\n",
    "    print( '%s -> %f' % (tweet, predict_tweet(tweet, freqs, theta)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You should get:\n",
    "\n",
    "```\n",
    "I am happy -> 0.518580\n",
    "I am bad -> 0.494339\n",
    "this movie should have been great. -> 0.515331\n",
    "great -> 0.515464\n",
    "great great -> 0.530898\n",
    "great great great -> 0.546273\n",
    "great great great great -> 0.561561\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test with your own tweet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predict_tweet' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-575e0df675d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmy_tweet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'I ate a cow'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpredict_tweet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_tweet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfreqs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'predict_tweet' is not defined"
     ]
    }
   ],
   "source": [
    "my_tweet = 'I ate a cow'\n",
    "predict_tweet(my_tweet, freqs, theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check performance using the test set\n",
    "After training your model using the training set above, check how your model might perform on real, unseen data, by testing it against the test set.\n",
    "\n",
    "#### Instructions: Implement `test_logistic_regression` \n",
    "* Given the test data and the weights of your trained model, calculate the accuracy of your logistic regression model. \n",
    "* Use your `predict_tweet()` function to make predictions on each tweet in the test set.\n",
    "* If the prediction is > 0.5, set the model's classification `y_hat` to 1, otherwise set the model's classification `y_hat` to 0.\n",
    "* A prediction is accurate when `y_hat` equals `test_y`.  Sum up all the instances when they are equal and divide by `m`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PS:\n",
    "* You can use [np.asarray()](https://numpy.org/doc/stable/reference/generated/numpy.asarray.html) to convert a lists to numpy array.\n",
    "* You can use [np.squeeze()](https://numpy.org/doc/stable/reference/generated/numpy.squeeze.html) to make an (m,1) dimensional array into an (m,) array.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_logistic_regression(test_x, test_y, freqs, theta):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "        test_x: a list of tweets\n",
    "        test_y: (m, 1) vector with the corresponding labels for the list of tweets\n",
    "        freqs: a dictionary with the frequency of each pair (or tuple)\n",
    "        theta: weight vector of dimension (3, 1)\n",
    "    Output: \n",
    "        accuracy: (# of tweets classified correctly) / (total # of tweets)\n",
    "    \"\"\"\n",
    "        \n",
    "    # the list for storing predictions\n",
    "    y_hat = []\n",
    "    \n",
    "    for tweet in test_x:\n",
    "        # get the label prediction for the tweet\n",
    "        y_pred = predict_tweet(tweet, freqs, theta)## ADD YOUR CODE HERE\n",
    "        \n",
    "        if y_pred > 0.5:\n",
    "            # append 1.0 to the list\n",
    "            y_hat.append([1.0])\n",
    "        else:\n",
    "            # append 0 to the list\n",
    "            ## ADD YOUR CODE HERE\n",
    "            y_hat.append([0.0])\n",
    "\n",
    "\n",
    "    # With the above implementation, y_hat is a list, but test_y is (m,1) array\n",
    "    # convert both to one-dimensional arrays in order to compare them using the '==' operator\n",
    "    accuracy = np.mean(np.equal(test_y.reshape(-1,1),y_hat))\n",
    "\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression model accuracy : 0.9950\n"
     ]
    }
   ],
   "source": [
    "tmp_accuracy = test_logistic_regression(test_x, test_y, freqs, theta)\n",
    "print(f'Logistic regression model accuracy : {tmp_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You should get:\n",
    "\n",
    "```0.9950```  \n",
    "Not bad for such a simple model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error Analysis\n",
    "\n",
    "Now lets check some misclassifications.\n",
    "Can you think about reasons for those errors?\n",
    "What those tweets have in common?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Predicted Tweet\n",
      "__________________________________________\n",
      "THE TWEET IS: @jaredNOTsubway @iluvmariah @Bravotv Then that truly is a LATERAL move! Now, we all know the Queen Bee is UPWARD BOUND : ) #MovingOnUp\n",
      "THE PROCESSED TWEET IS:\n",
      "['truli', 'later', 'move', 'know', 'queen', 'bee', 'upward', 'bound', 'movingonup']\n",
      "1\t0.49996897\tb'truli later move know queen bee upward bound movingonup'\n",
      "__________________________________________\n",
      "THE TWEET IS: @MarkBreech Not sure it would be good thing 4 my bottom daring 2 say 2 Miss B but Im gonna be so stubborn on mouth soaping ! #NotHavingit :p\n",
      "THE PROCESSED TWEET IS:\n",
      "['sure', 'would', 'good', 'thing', '4', 'bottom', 'dare', '2', 'say', '2', 'miss', 'b', 'im', 'gonna', 'stubborn', 'mouth', 'soap', 'nothavingit', ':p']\n",
      "1\t0.48650628\tb'sure would good thing 4 bottom dare 2 say 2 miss b im gonna stubborn mouth soap nothavingit :p'\n",
      "__________________________________________\n",
      "THE TWEET IS: I'm playing Brain Dots : ) #BrainDots\n",
      "http://t.co/UGQzOx0huu\n",
      "THE PROCESSED TWEET IS:\n",
      "[\"i'm\", 'play', 'brain', 'dot', 'braindot']\n",
      "1\t0.48370676\tb\"i'm play brain dot braindot\"\n",
      "__________________________________________\n",
      "THE TWEET IS: I'm playing Brain Dots : ) #BrainDots http://t.co/aOKldo3GMj http://t.co/xWCM9qyRG5\n",
      "THE PROCESSED TWEET IS:\n",
      "[\"i'm\", 'play', 'brain', 'dot', 'braindot']\n",
      "1\t0.48370676\tb\"i'm play brain dot braindot\"\n",
      "__________________________________________\n",
      "THE TWEET IS: I'm playing Brain Dots : ) #BrainDots http://t.co/R2JBO8iNww http://t.co/ow5BBwdEMY\n",
      "THE PROCESSED TWEET IS:\n",
      "[\"i'm\", 'play', 'brain', 'dot', 'braindot']\n",
      "1\t0.48370676\tb\"i'm play brain dot braindot\"\n",
      "__________________________________________\n",
      "THE TWEET IS: off to the park to get some sunlight : )\n",
      "THE PROCESSED TWEET IS:\n",
      "['park', 'get', 'sunlight']\n",
      "1\t0.49578773\tb'park get sunlight'\n",
      "__________________________________________\n",
      "THE TWEET IS: @msarosh Uff Itna Miss karhy thy ap :p\n",
      "THE PROCESSED TWEET IS:\n",
      "['uff', 'itna', 'miss', 'karhi', 'thi', 'ap', ':p']\n",
      "1\t0.48199817\tb'uff itna miss karhi thi ap :p'\n",
      "__________________________________________\n",
      "THE TWEET IS: @phenomyoutube u probs had more fun with david than me : (\n",
      "THE PROCESSED TWEET IS:\n",
      "['u', 'prob', 'fun', 'david']\n",
      "0\t0.50020361\tb'u prob fun david'\n",
      "__________________________________________\n",
      "THE TWEET IS: pats jay : (\n",
      "THE PROCESSED TWEET IS:\n",
      "['pat', 'jay']\n",
      "0\t0.50039294\tb'pat jay'\n",
      "__________________________________________\n",
      "THE TWEET IS: my beloved grandmother : ( https://t.co/wt4oXq5xCf\n",
      "THE PROCESSED TWEET IS:\n",
      "['belov', 'grandmoth']\n",
      "0\t0.50000002\tb'belov grandmoth'\n"
     ]
    }
   ],
   "source": [
    "# Some error analysis done for you\n",
    "print('Label Predicted Tweet')\n",
    "for x,y in zip(test_x,test_y):\n",
    "    y_hat = predict_tweet(x, freqs, theta)\n",
    "\n",
    "    if np.abs(y - (y_hat > 0.5)) > 0:\n",
    "        print(f'__________________________________________')\n",
    "        print(f'THE TWEET IS: {x}')\n",
    "        print(f'THE PROCESSED TWEET IS:\\n{process_tweet(x)}')\n",
    "        print('%d\\t%0.8f\\t%s' % (y, y_hat, ' '.join(process_tweet(x)).encode('ascii', 'ignore')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try to think about it and discuss about the issue in you Report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict your own text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hate', 'sushi', 'pleas', 'bring', 'meat']\n",
      "[[0.47201289]]\n",
      "Negative.\n"
     ]
    }
   ],
   "source": [
    "# Test your own texts here\n",
    "my_tweet = 'I hate sushi, please bring me some meat.'\n",
    "print(process_tweet(my_tweet))\n",
    "y_hat = predict_tweet(my_tweet, freqs, theta)\n",
    "print(y_hat)\n",
    "if y_hat > 0.5:\n",
    "    print('Positive.')\n",
    "else: \n",
    "    print('Negative.')"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "schema_names": [
    "NLPC1-1"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
